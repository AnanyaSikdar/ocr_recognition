{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-01T09:59:18.280404Z",
          "iopub.status.busy": "2025-03-01T09:59:18.280113Z",
          "iopub.status.idle": "2025-03-01T09:59:18.284504Z",
          "shell.execute_reply": "2025-03-01T09:59:18.283782Z",
          "shell.execute_reply.started": "2025-03-01T09:59:18.280384Z"
        },
        "id": "VKu6QwMCRTdh",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.optim.lr_scheduler import StepLR"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vHFtYbZ4Rybb"
      },
      "source": [
        "#### **MNIST Dataset: The Classic Benchmark** \n",
        "\n",
        "**MNIST** is a popular dataset of **70,000 handwritten digits** (0-9) widely used for training and testing image processing systems.  \n",
        "\n",
        "ðŸ“Š **Dataset Overview:**  \n",
        "- ðŸ–¼ï¸ **60,000 Training Images**  \n",
        "- ðŸ–¼ï¸ **10,000 Test Images**  \n",
        "- ðŸ”² **Image Size:** 28x28 pixels (grayscale)  \n",
        "- ðŸ” **Digits:** Size-normalized and centered  \n",
        "- Check out the full details here: [ðŸ“– Wikipedia - MNIST Database](https://en.wikipedia.org/wiki/MNIST_database)  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-01T09:59:18.288976Z",
          "iopub.status.busy": "2025-03-01T09:59:18.288736Z",
          "iopub.status.idle": "2025-03-01T09:59:18.305817Z",
          "shell.execute_reply": "2025-03-01T09:59:18.305182Z",
          "shell.execute_reply.started": "2025-03-01T09:59:18.288948Z"
        },
        "id": "CwfbcGAeRY0B",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# **Part 1: Data Loading and Preprocessing**\n",
        "# TODO: Complete the data loading code \n",
        "\n",
        "def load_mnist_data(batch_size=16):\n",
        "    transform = transforms.Compose([\n",
        "        transforms.RandomRotation(10),\n",
        "        transforms.RandomAffine(0,translate=(0.05,0.05)),\n",
        "        transforms.Resize((224,224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.1307,), (0.3081,))\n",
        "    ])\n",
        "\n",
        "    train_dataset = torchvision.datasets.MNIST(root='./data', train=True,  transform=transform, download=True)\n",
        "    test_dataset  = torchvision.datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    return train_loader, test_loader  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eEKhzgBHTa8n"
      },
      "source": [
        "#### **Understanding Dropout**\n",
        "\n",
        "**Dropout** is a regularization technique ðŸ›¡ï¸ used to prevent **overfitting** in deep neural networks by randomly ignoring or **\"dropping out\"** some layer outputs during training.  \n",
        "\n",
        "##### ðŸ” **Where is Dropout Applied?**  \n",
        "Dropout can be implemented in different types of layers:  \n",
        "- **Dense (Fully Connected) Layers**  \n",
        "- **Convolutional Layers**  \n",
        "- **Recurrent Layers**  \n",
        "- It is **NOT applied** to the output layer!  \n",
        "\n",
        "##### ðŸŽ² **How Dropout Works**  \n",
        "The **dropout probability** ðŸ”¢ determines how likely it is for a neuron to be dropped out:  \n",
        "- **Input Layer:** Lower dropout probability  \n",
        "- **Hidden Layers:** Higher dropout probability  \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-01T09:59:18.307052Z",
          "iopub.status.busy": "2025-03-01T09:59:18.306815Z",
          "iopub.status.idle": "2025-03-01T09:59:18.327937Z",
          "shell.execute_reply": "2025-03-01T09:59:18.327297Z",
          "shell.execute_reply.started": "2025-03-01T09:59:18.307033Z"
        },
        "id": "lzh30yJGRcW0",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# ðŸš€ **Part 2: Custom Dropout Implementation**\n",
        "\n",
        "class CustomDropout(nn.Module):\n",
        "    def __init__(self, p=0.5):\n",
        "        super(CustomDropout, self).__init__()\n",
        "        self.p = p\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.training:\n",
        "            # ðŸ‹ï¸â€â™‚ï¸ Drop units only during training mode\n",
        "            mask = torch.rand_like(x) > self.p\n",
        "            scale = 1.0 / (1.0 - self.p) if self.p != 1.0 else 0.0\n",
        "            x = x * mask.float()\n",
        "            x = x * scale\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZlosZCG9UdJW"
      },
      "source": [
        "#### **Batch Normalization for 2D Inputs**\n",
        "\n",
        "BatchNorm2d normalizes inputs across a batch during training, accelerating training and improving generalization.\n",
        "\n",
        "##### **Benefits**\n",
        "\n",
        "-   Reduces internal covariate shift by normalizing activations.\n",
        "-   Accelerates training.\n",
        "-   Acts as a regularizer, potentially reducing the need for dropout.\n",
        "\n",
        "##### **How It Works**\n",
        "\n",
        "1.  Computes the mean and variance for each feature map across the batch.\n",
        "2.  Normalizes activations by subtracting the mean and dividing by the standard deviation.\n",
        "3.  Applies learnable scale (Î³) and shift (Î²) parameters for flexibility.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-01T09:59:18.329566Z",
          "iopub.status.busy": "2025-03-01T09:59:18.329331Z",
          "iopub.status.idle": "2025-03-01T09:59:18.347741Z",
          "shell.execute_reply": "2025-03-01T09:59:18.347173Z",
          "shell.execute_reply.started": "2025-03-01T09:59:18.329547Z"
        },
        "id": "7EY35E-ARfwn",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# ðŸš€ **Part 3: Custom BatchNorm2d Implementation**\n",
        "\n",
        "class CustomBatchNorm2d(nn.Module):\n",
        "  \"\"\"\n",
        "  ðŸ› ï¸ Custom 2D Batch Normalization Layer ðŸ”„\n",
        "\n",
        "  ðŸ“œ **Requirements:**\n",
        "  1ï¸âƒ£ Initialize **running mean**, **variance**, **gamma (scale)**, and **beta (shift)** âš–ï¸\n",
        "  2ï¸âƒ£ Implement **forward pass** with proper normalization âœ¨\n",
        "  3ï¸âƒ£ Track **running statistics** during training ðŸ“Š\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, num_features, eps=1e-5, momentum=0.1):\n",
        "    super(CustomBatchNorm2d, self).__init__()\n",
        "    self.num_features = num_features\n",
        "    self.momentum = momentum\n",
        "    self.eps = eps\n",
        "\n",
        "    self.register_buffer('running_mean', torch.zeros(num_features))\n",
        "    self.register_buffer('running_var', torch.ones(num_features))\n",
        "    self.gamma = nn.Parameter(torch.ones(num_features))\n",
        "    self.beta = nn.Parameter(torch.zeros(num_features))\n",
        "\n",
        "  def forward(self, x):\n",
        "    if self.training:\n",
        "      # Calculate batch mean and variance\n",
        "      batch_mean__ = torch.mean(x, dim = (0, 2, 3))\n",
        "      batch_var___ = torch.var(x, dim = (0, 2, 3), unbiased = False)\n",
        "\n",
        "      self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * batch_mean__\n",
        "      self.running_var = (1 - self.momentum) * self.running_var + self.momentum * batch_var___\n",
        "      mean = batch_mean__\n",
        "      var = batch_var___\n",
        "\n",
        "    else:\n",
        "      mean = self.running_mean\n",
        "      var = self.running_var\n",
        "\n",
        "    # Reshape for broadcasting\n",
        "    mean  = mean.view(1, self.num_features, 1, 1)\n",
        "    var   = var.view(1, self.num_features, 1, 1)\n",
        "    gamma = self.gamma.view(1, self.num_features, 1, 1)\n",
        "    beta  = self.beta.view(1, self.num_features, 1, 1)\n",
        "\n",
        "    # Normalize the input\n",
        "    x_norm = (x - mean) / torch.sqrt(var + self.eps)\n",
        "    x_out = gamma * x_norm + beta\n",
        "    return x_out\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cvXwM-45VOrF"
      },
      "source": [
        "#### **Activation Functions**\n",
        "\n",
        "**Activation functions** introduce **non-linearity** into neural networks, allowing them to learn and solve **complex tasks**. Without them, the network would only be able to learn **linear relationships** (which isnâ€™t very useful for most problems ðŸ˜…).  \n",
        "\n",
        "---\n",
        "\n",
        "#### ðŸŒŸ **Why are Activation Functions Important?**  \n",
        "- ðŸ”„ **Adds non-linearity** to the model  \n",
        "- ðŸš€ **Enables learning of complex patterns**  \n",
        "- ðŸ› ï¸ Helps the network make decisions, just like neurons in the brain ðŸ§   \n",
        "\n",
        "---\n",
        "\n",
        "#### ðŸ“š **Common Activation Functions**  \n",
        "\n",
        "##### 1ï¸âƒ£ **ReLU (Rectified Linear Unit)** âš¡  \n",
        "- **Formula:** `f(x) = max(0, x)`  \n",
        "- **Use:** Most common in hidden layers of CNNs and DNNs  \n",
        "- **Pros:** Simple, fast, reduces vanishing gradient problem  \n",
        "- **Cons:** Can suffer from the **dying ReLU** problem ðŸ˜µ  \n",
        "\n",
        "---\n",
        "\n",
        "##### 2ï¸âƒ£ **Sigmoid (Logistic Function)** ðŸ“ˆ  \n",
        "- **Formula:** `f(x) = 1 / (1 + exp(-x))`  \n",
        "- **Range:** (0, 1)  \n",
        "- **Use:** Good for binary classification tasks ðŸ”¢  \n",
        "- **Cons:** Can cause **vanishing gradients** ðŸ˜¬  \n",
        "\n",
        "---\n",
        "\n",
        "##### 3ï¸âƒ£ **Tanh (Hyperbolic Tangent)** ðŸ”„  \n",
        "- **Formula:** `f(x) = (2 / (1 + exp(-2x))) - 1`  \n",
        "- **Range:** (-1, 1)  \n",
        "- **Use:** Often used in recurrent networks ðŸ”  \n",
        "- **Pros:** Centered around 0, making optimization easier  \n",
        "- **Cons:** Also prone to vanishing gradients  \n",
        "\n",
        "---\n",
        "\n",
        "##### 4ï¸âƒ£ **Softmax** ðŸŽ¯  \n",
        "- **Formula:** Converts logits into probabilities ðŸ”¢  \n",
        "- **Range:** (0, 1), sums to 1  \n",
        "- **Use:** Last layer for multi-class classification problems  \n",
        "\n",
        "---\n",
        "\n",
        "#### ðŸŽ¯ **How to Choose an Activation Function?**  \n",
        "- For **hidden layers**, use **ReLU** or its variants (like Leaky ReLU or ELU)  \n",
        "- For **binary classification**, use **Sigmoid**  \n",
        "- For **multi-class classification**, use **Softmax**  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-01T09:59:18.348805Z",
          "iopub.status.busy": "2025-03-01T09:59:18.348557Z",
          "iopub.status.idle": "2025-03-01T09:59:18.369232Z",
          "shell.execute_reply": "2025-03-01T09:59:18.368581Z",
          "shell.execute_reply.started": "2025-03-01T09:59:18.348779Z"
        },
        "id": "Kxs-RWzbhYB9",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "class CustomReLU(nn.Module):\n",
        "    def forward(self, x):\n",
        "        ReLU =  torch.max(x, torch.tensor(0.0))\n",
        "        return ReLU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-01T09:59:18.370428Z",
          "iopub.status.busy": "2025-03-01T09:59:18.370186Z",
          "iopub.status.idle": "2025-03-01T09:59:18.392146Z",
          "shell.execute_reply": "2025-03-01T09:59:18.391531Z",
          "shell.execute_reply.started": "2025-03-01T09:59:18.370410Z"
        },
        "id": "6LOf8k6ohaLf",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "class CustomMaxPooling2d(nn.Module):\n",
        "    def __init__(self, kernel_size=2, stride=2):\n",
        "        super(CustomMaxPooling2d, self).__init__()\n",
        "        self.kernel_size = kernel_size\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        # ðŸ”„ **TODO: Implement forward pass for max-pooling**\n",
        "        # Hint: Use `unfold` to break the input into windows and compute the max for each window ðŸ”\n",
        "        batch_size = x.size(0)\n",
        "        channel = x.size(1)\n",
        "        height = x.size(2)\n",
        "        width = x.size(3)\n",
        "        stride__ = self.stride if height > 3 else 1\n",
        "\n",
        "        k = self.kernel_size\n",
        "        s = stride__\n",
        "\n",
        "        x = x.unfold(2, k, s)\n",
        "        x = x.unfold(3, k, s)\n",
        "        x = x.contiguous().view(x.size(0), x.size(1), -1, k * k)\n",
        "        x_pooled, _ = x.max(dim = -1)\n",
        "\n",
        "        out_height = (height - k) // s + 1\n",
        "        out_width = (width - k) // s + 1\n",
        "        x_pooled = x_pooled.view(batch_size, channel, out_height, out_width)\n",
        "        return x_pooled\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5eNoFsCo5L_"
      },
      "source": [
        "#### **MaxPooling with `unfold`**\n",
        "\n",
        "`torch.nn.functional.unfold` is a tool that breaks an input tensor into sliding windows. Here's how to use it for max pooling:\n",
        "\n",
        "1.  **Unfold the input tensor:** This provides overlapping windows based on the `kernel_size` and `stride`.\n",
        "\n",
        "    ```python\n",
        "    # Unfold the input tensor into sliding windows\n",
        "    x_unfolded = x.unfold(2, self.kernel_size, self.stride).unfold(3, self.kernel_size, self.stride)\n",
        "    ```\n",
        "2.  **Reshape the unfolded tensor:** Convert it to a shape that facilitates maximum computation.\n",
        "\n",
        "    ```python\n",
        "    x_unfolded = x_unfolded.contiguous().view(x.size(0), x.size(1), -1, self.kernel_size * self.kernel_size)\n",
        "    ```\n",
        "3.  **Compute the max along the last dimension:** This dimension represents all elements within each window.\n",
        "\n",
        "    ```python\n",
        "    x_pooled, _ = x_unfolded.max(dim=-1)\n",
        "    ```\n",
        "4.  **Reshape back to the correct output size:** This ensures the output matches a typical max pooling layer.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_2DAv2wGi0U6"
      },
      "source": [
        "#### **Implement Your Own Custom VGG16 Model**\n",
        "\n",
        "1.  Implement both `CustomReLU` and `CustomMaxPooling2d` in the provided classes.\n",
        "2.  Use only tensor operations like `torch.max` or `unfold` (no built-in `F.relu` or `nn.MaxPool2d`).\n",
        "3.  Integrate them into your `CustomVGG16` model.   \n",
        "4.  Check out the full details here: [VGG16 Paper](https://arxiv.org/pdf/1409.1556)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-01T09:59:18.482911Z",
          "iopub.status.busy": "2025-03-01T09:59:18.482688Z",
          "iopub.status.idle": "2025-03-01T09:59:18.493361Z",
          "shell.execute_reply": "2025-03-01T09:59:18.492358Z",
          "shell.execute_reply.started": "2025-03-01T09:59:18.482894Z"
        },
        "id": "YQ1qSSQzRigE",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# ðŸš€ **Part 4: Custom VGG16 Model Implementation**\n",
        "class CustomVGG16(nn.Module):\n",
        "\n",
        "    def __init__(self, num_classes=10):  # num_classes = 10 for MNIST\n",
        "        super(CustomVGG16, self).__init__()\n",
        "\n",
        "        self.features = nn.Sequential(\n",
        "            # Block 1\n",
        "            nn.Conv2d(1, 64, kernel_size=3, padding=1),\n",
        "            CustomBatchNorm2d(64),\n",
        "            CustomReLU(),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
        "            CustomBatchNorm2d(64),\n",
        "            CustomReLU(),\n",
        "            CustomMaxPooling2d(kernel_size=2, stride=2),\n",
        "\n",
        "            # Block 2\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            CustomBatchNorm2d(128),\n",
        "            CustomReLU(),\n",
        "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
        "            CustomBatchNorm2d(128),\n",
        "            CustomReLU(),\n",
        "            CustomMaxPooling2d(kernel_size=2, stride=2),\n",
        "\n",
        "            # Block 3\n",
        "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
        "            CustomBatchNorm2d(256),\n",
        "            CustomReLU(),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            CustomBatchNorm2d(256),\n",
        "            CustomReLU(),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            CustomBatchNorm2d(256),\n",
        "            CustomReLU(),\n",
        "            CustomMaxPooling2d(kernel_size=2, stride=2),\n",
        "\n",
        "            # Block 4\n",
        "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
        "            CustomBatchNorm2d(512),\n",
        "            CustomReLU(),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            CustomBatchNorm2d(512),\n",
        "            CustomReLU(),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            CustomBatchNorm2d(512),\n",
        "            CustomReLU(),\n",
        "            CustomMaxPooling2d(kernel_size=2, stride=2),\n",
        "\n",
        "            # Block 5\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            CustomBatchNorm2d(512),\n",
        "            CustomReLU(),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            CustomBatchNorm2d(512),\n",
        "            CustomReLU(),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            CustomBatchNorm2d(512),\n",
        "            CustomReLU(),\n",
        "            CustomMaxPooling2d(kernel_size=2, stride=2),\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            CustomDropout(0.5),\n",
        "            nn.Linear(512 * 7 * 7, 4096),\n",
        "            CustomReLU(),\n",
        "            CustomDropout(0.5),\n",
        "            nn.Linear(4096, 4096),\n",
        "            CustomReLU(),\n",
        "            CustomDropout(0.5),\n",
        "            nn.Linear(4096, num_classes)\n",
        "        )\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                if m.bias is not None: nn.init.constant_(m.bias, 0)\n",
        "\n",
        "            elif isinstance(m, CustomBatchNorm2d):\n",
        "                nn.init.constant_(m.gamma, 1)\n",
        "                nn.init.constant_(m.beta, 0)\n",
        "\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.normal_(m.weight, 0, 0.01)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-01T09:59:18.494626Z",
          "iopub.status.busy": "2025-03-01T09:59:18.494426Z",
          "iopub.status.idle": "2025-03-01T09:59:18.515390Z",
          "shell.execute_reply": "2025-03-01T09:59:18.514586Z",
          "shell.execute_reply.started": "2025-03-01T09:59:18.494608Z"
        },
        "id": "vB-u6IalRk4q",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# ðŸš€ **Part 5: Training Functions**\n",
        "\n",
        "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
        "    \"\"\"\n",
        "    Implement training loop for one epoch\n",
        "    \"\"\"\n",
        "    model.train()                                                                       # Switch to training mode\n",
        "    running_loss = 0.0                                                                  # Track the cumulative loss\n",
        "    correct = 0                                                                         # Correct predictions counter\n",
        "    total = 0                                                                           # Total samples counter\n",
        "\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):                            # Loop through batches\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(data)\n",
        "        loss = criterion(outputs, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += target.size(0)\n",
        "        correct += (predicted == target).sum().item()\n",
        "\n",
        "        if (batch_idx + 1) % 300 == 0:\n",
        "            print(f\"Batch {batch_idx + 1}/{len(train_loader):<5} | \"\n",
        "                  f\"{'Loss:':<6}{loss.item():<8.4f} | \"\n",
        "                  f\"{'Accuracy:':<9}{(100. * correct / total):>6.2f}%\")\n",
        "\n",
        "    # Return average loss and accuracy for the epoch\n",
        "    return running_loss / len(train_loader), 100. * correct / total\n",
        "\n",
        "def evaluate(model, test_loader, criterion, device):\n",
        "    \"\"\"\n",
        "    Implement evaluation loop\n",
        "    \"\"\"\n",
        "    model.eval()                                                                        # Switch to evaluation mode (no gradients)\n",
        "    test_loss = 0                                                                       # Track cumulative test loss\n",
        "    correct = 0                                                                         # Correct predictions counter\n",
        "    total = 0                                                                           # Total samples counter\n",
        "\n",
        "    with torch.no_grad():                                                               \n",
        "        # Your code here (e.g., forward pass, loss calculation, accuracy calculation)\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            outputs = model(data)\n",
        "            loss = criterion(outputs, target)\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += target.size(0)\n",
        "            correct += (predicted == target).sum().item()\n",
        "\n",
        "    # Return average test loss and accuracy\n",
        "    return test_loss / len(test_loader), 100. * correct / total\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-01T09:59:18.517062Z",
          "iopub.status.busy": "2025-03-01T09:59:18.516760Z",
          "iopub.status.idle": "2025-03-01T09:59:18.539851Z",
          "shell.execute_reply": "2025-03-01T09:59:18.539306Z",
          "shell.execute_reply.started": "2025-03-01T09:59:18.517030Z"
        },
        "id": "r9Tnydj-RpO5",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Part 6: Main Training Loop\n",
        "\n",
        "def main():\n",
        "  # Hyperparameters\n",
        "  BATCH_SIZE = 16             # Batch size for data loading\n",
        "  EPOCHS = 10                 # Number of training epochs\n",
        "  LEARNING_RATE = 0.001       # Learning rate for optimizer\n",
        "  DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  # Use GPU if available\n",
        "\n",
        "  # Load data\n",
        "  train_loader, test_loader = load_mnist_data(BATCH_SIZE)\n",
        "\n",
        "  # Initialize model, criterion, optimizer\n",
        "  model = CustomVGG16().to(DEVICE)  # Move model to the selected device\n",
        "  criterion = nn.CrossEntropyLoss()  # Loss function for classification\n",
        "  optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)  # Adam optimizer for better convergence\n",
        "  scheduler = StepLR(optimizer, step_size=2, gamma=0.1)\n",
        "\n",
        "\n",
        "  # Training loop\n",
        "  train_losses = []  # Track training losses\n",
        "  test_losses = []  # Track test losses\n",
        "  train_accs = []  # Track training accuracy\n",
        "  test_accs = []  # Track test accuracy\n",
        "\n",
        "  for epoch in range(EPOCHS):\n",
        "    print(f\"Epoch {epoch+1}/{EPOCHS}\")  # Display current epoch\n",
        "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, DEVICE)\n",
        "    test_loss, test_acc = evaluate(model, test_loader, criterion, DEVICE)\n",
        "    train_accs.append(train_acc)\n",
        "    train_losses.append(train_loss)\n",
        "    test_accs.append(test_acc)\n",
        "    test_losses.append(test_loss)\n",
        "    scheduler.step()\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(f\" Epoch {epoch+1} Training Summary \".center(50))\n",
        "    print(\"=\"*50)\n",
        "    print(f\"{'Train Loss':<20}{train_loss:>10.4f}\")\n",
        "    print(f\"{'Train Accuracy':<20}{train_acc:>9.2f}%\")\n",
        "    print(f\"{'Test Loss':<20}{test_loss:>10.4f}\")\n",
        "    print(f\"{'Test Accuracy':<20}{test_acc:>9.2f}%\")\n",
        "    print(\"=\"*50 + \"\\n\")\n",
        "\n",
        "\n",
        "  # Plot results\n",
        "\n",
        "  plt.figure(figsize=(12, 5))\n",
        "\n",
        "  # Loss Plot\n",
        "  plt.subplot(1, 2, 1)\n",
        "  plt.plot(train_losses, label=\"Train Loss\", color=\"blue\", linewidth=2)\n",
        "  plt.plot(test_losses, label=\"Test Loss\", color=\"red\", linewidth=2, linestyle=\"--\")\n",
        "  plt.xlabel(\"Epochs\", fontsize=12)\n",
        "  plt.ylabel(\"Loss\", fontsize=12)\n",
        "  plt.title(\"Training vs. Testing Loss\", fontsize=14, fontweight=\"bold\")\n",
        "  plt.legend(fontsize=10)\n",
        "  plt.grid(alpha=0.3)\n",
        "\n",
        "  min_test_loss_epoch = test_losses.index(min(test_losses))\n",
        "  plt.scatter(min_test_loss_epoch, min(test_losses), color=\"red\", s=50, label=\"Min Test Loss\")\n",
        "  plt.legend()\n",
        "\n",
        "  # Accuracy Plot\n",
        "  plt.subplot(1, 2, 2)\n",
        "  plt.plot(train_accs, label=\"Train Accuracy\", color=\"green\", linewidth=2)\n",
        "  plt.plot(test_accs, label=\"Test Accuracy\", color=\"orange\", linewidth=2, linestyle=\"--\")\n",
        "  plt.xlabel(\"Epochs\", fontsize=12)\n",
        "  plt.ylabel(\"Accuracy\", fontsize=12)\n",
        "  plt.title(\"Training vs. Testing Accuracy\", fontsize=14, fontweight=\"bold\")\n",
        "  plt.legend(fontsize=10)\n",
        "  plt.grid(alpha=0.3)\n",
        "\n",
        "  max_test_acc_epoch = test_accs.index(max(test_accs))\n",
        "  plt.scatter(max_test_acc_epoch, max(test_accs), color=\"orange\", s=50, label=\"Max Test Accuracy\")\n",
        "  plt.legend()\n",
        "\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "execution": {
          "iopub.execute_input": "2025-03-01T09:59:26.057257Z",
          "iopub.status.busy": "2025-03-01T09:59:26.056928Z"
        },
        "id": "0UqF5DyiFWRb",
        "outputId": "76970303-ff55-4017-8e3a-80a8e52211bd",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸŒŸ Epoch 1/10\n",
            "Batch 300/3750  | Loss: 1.5471   | Accuracy: 23.21%\n",
            "Batch 600/3750  | Loss: 0.7590   | Accuracy: 31.62%\n",
            "Batch 900/3750  | Loss: 0.7773   | Accuracy: 38.03%\n",
            "Batch 1200/3750  | Loss: 0.7575   | Accuracy: 43.70%\n",
            "Batch 1500/3750  | Loss: 0.3569   | Accuracy: 48.46%\n",
            "Batch 1800/3750  | Loss: 0.6805   | Accuracy: 52.02%\n",
            "Batch 2100/3750  | Loss: 0.4728   | Accuracy: 56.01%\n",
            "Batch 2400/3750  | Loss: 0.9284   | Accuracy: 59.23%\n",
            "Batch 2700/3750  | Loss: 0.2107   | Accuracy: 62.01%\n",
            "Batch 3000/3750  | Loss: 0.3379   | Accuracy: 64.67%\n",
            "Batch 3300/3750  | Loss: 1.0562   | Accuracy: 67.05%\n",
            "Batch 3600/3750  | Loss: 0.1389   | Accuracy: 69.18%\n",
            "\n",
            "==================================================\n",
            "        \u001b[1m Epoch 1 Training Summary \u001b[0m        \n",
            "==================================================\n",
            "Train Loss              0.8547\n",
            "Train Accuracy          70.16%\n",
            "Test Loss               0.4182\n",
            "Test Accuracy           92.01%\n",
            "==================================================\n",
            "\n",
            "ðŸŒŸ Epoch 2/10\n",
            "Batch 300/3750  | Loss: 0.8573   | Accuracy: 92.31%\n",
            "Batch 600/3750  | Loss: 0.2074   | Accuracy: 92.76%\n",
            "Batch 900/3750  | Loss: 0.1559   | Accuracy: 92.70%\n",
            "Batch 1200/3750  | Loss: 0.0344   | Accuracy: 93.26%\n",
            "Batch 1500/3750  | Loss: 0.0654   | Accuracy: 93.50%\n",
            "Batch 1800/3750  | Loss: 0.0008   | Accuracy: 93.75%\n",
            "Batch 2100/3750  | Loss: 0.9015   | Accuracy: 93.92%\n",
            "Batch 2400/3750  | Loss: 0.0023   | Accuracy: 93.90%\n",
            "Batch 2700/3750  | Loss: 0.4372   | Accuracy: 93.85%\n",
            "Batch 3000/3750  | Loss: 0.2114   | Accuracy: 94.07%\n",
            "Batch 3300/3750  | Loss: 0.0288   | Accuracy: 93.98%\n",
            "Batch 3600/3750  | Loss: 0.3426   | Accuracy: 94.05%\n",
            "\n",
            "==================================================\n",
            "        \u001b[1m Epoch 2 Training Summary \u001b[0m        \n",
            "==================================================\n",
            "Train Loss              0.2910\n",
            "Train Accuracy          94.07%\n",
            "Test Loss               0.1115\n",
            "Test Accuracy           97.69%\n",
            "==================================================\n",
            "\n",
            "ðŸŒŸ Epoch 3/10\n",
            "Batch 300/3750  | Loss: 0.0402   | Accuracy: 96.56%\n",
            "Batch 600/3750  | Loss: 0.5674   | Accuracy: 96.88%\n",
            "Batch 900/3750  | Loss: 0.0815   | Accuracy: 97.03%\n",
            "Batch 1200/3750  | Loss: 0.0101   | Accuracy: 97.15%\n",
            "Batch 1500/3750  | Loss: 0.1899   | Accuracy: 97.25%\n",
            "Batch 1800/3750  | Loss: 0.8824   | Accuracy: 97.28%\n",
            "Batch 2100/3750  | Loss: 0.1145   | Accuracy: 97.32%\n",
            "Batch 2400/3750  | Loss: 0.0438   | Accuracy: 97.39%\n",
            "Batch 2700/3750  | Loss: 0.1261   | Accuracy: 97.45%\n",
            "Batch 3000/3750  | Loss: 0.0010   | Accuracy: 97.54%\n",
            "Batch 3300/3750  | Loss: 0.2155   | Accuracy: 97.59%\n",
            "Batch 3600/3750  | Loss: 0.0037   | Accuracy: 97.59%\n",
            "\n",
            "==================================================\n",
            "        \u001b[1m Epoch 3 Training Summary \u001b[0m        \n",
            "==================================================\n",
            "Train Loss              0.1091\n",
            "Train Accuracy          97.61%\n",
            "Test Loss               0.0567\n",
            "Test Accuracy           98.75%\n",
            "==================================================\n",
            "\n",
            "ðŸŒŸ Epoch 4/10\n",
            "Batch 300/3750  | Loss: 0.0338   | Accuracy: 98.23%\n",
            "Batch 600/3750  | Loss: 0.0485   | Accuracy: 98.26%\n",
            "Batch 900/3750  | Loss: 0.0041   | Accuracy: 98.19%\n",
            "Batch 1200/3750  | Loss: 0.0721   | Accuracy: 98.16%\n",
            "Batch 1500/3750  | Loss: 0.0088   | Accuracy: 98.18%\n",
            "Batch 1800/3750  | Loss: 0.0023   | Accuracy: 98.24%\n",
            "Batch 2100/3750  | Loss: 0.0045   | Accuracy: 98.24%\n",
            "Batch 2400/3750  | Loss: 0.0178   | Accuracy: 98.29%\n",
            "Batch 2700/3750  | Loss: 0.0030   | Accuracy: 98.29%\n",
            "Batch 3000/3750  | Loss: 0.0157   | Accuracy: 98.32%\n",
            "Batch 3300/3750  | Loss: 0.0026   | Accuracy: 98.30%\n",
            "Batch 3600/3750  | Loss: 0.0736   | Accuracy: 98.33%\n",
            "\n",
            "==================================================\n",
            "        \u001b[1m Epoch 4 Training Summary \u001b[0m        \n",
            "==================================================\n",
            "Train Loss              0.0754\n",
            "Train Accuracy          98.33%\n",
            "Test Loss               0.0468\n",
            "Test Accuracy           98.95%\n",
            "==================================================\n",
            "\n",
            "ðŸŒŸ Epoch 5/10\n",
            "Batch 300/3750  | Loss: 0.2679   | Accuracy: 98.42%\n",
            "Batch 600/3750  | Loss: 0.0805   | Accuracy: 98.31%\n",
            "Batch 900/3750  | Loss: 0.0005   | Accuracy: 98.42%\n",
            "Batch 1200/3750  | Loss: 0.0115   | Accuracy: 98.42%\n",
            "Batch 1500/3750  | Loss: 0.0013   | Accuracy: 98.45%\n",
            "Batch 1800/3750  | Loss: 0.0054   | Accuracy: 98.42%\n",
            "Batch 2100/3750  | Loss: 0.1513   | Accuracy: 98.40%\n",
            "Batch 2400/3750  | Loss: 0.0063   | Accuracy: 98.42%\n",
            "Batch 2700/3750  | Loss: 0.0220   | Accuracy: 98.42%\n",
            "Batch 3000/3750  | Loss: 0.0004   | Accuracy: 98.45%\n",
            "Batch 3300/3750  | Loss: 0.0160   | Accuracy: 98.47%\n",
            "Batch 3600/3750  | Loss: 0.1419   | Accuracy: 98.49%\n",
            "\n",
            "==================================================\n",
            "        \u001b[1m Epoch 5 Training Summary \u001b[0m        \n",
            "==================================================\n",
            "Train Loss              0.0662\n",
            "Train Accuracy          98.48%\n",
            "Test Loss               0.0429\n",
            "Test Accuracy           98.99%\n",
            "==================================================\n",
            "\n",
            "ðŸŒŸ Epoch 6/10\n",
            "Batch 300/3750  | Loss: 0.0040   | Accuracy: 98.73%\n",
            "Batch 600/3750  | Loss: 0.1797   | Accuracy: 98.51%\n",
            "Batch 900/3750  | Loss: 0.0031   | Accuracy: 98.57%\n",
            "Batch 1200/3750  | Loss: 0.0028   | Accuracy: 98.57%\n",
            "Batch 1500/3750  | Loss: 0.0002   | Accuracy: 98.53%\n",
            "Batch 1800/3750  | Loss: 0.0320   | Accuracy: 98.53%\n",
            "Batch 2100/3750  | Loss: 0.0078   | Accuracy: 98.54%\n",
            "Batch 2400/3750  | Loss: 0.0006   | Accuracy: 98.56%\n",
            "Batch 2700/3750  | Loss: 0.0095   | Accuracy: 98.58%\n",
            "Batch 3000/3750  | Loss: 0.0049   | Accuracy: 98.58%\n",
            "Batch 3300/3750  | Loss: 0.0001   | Accuracy: 98.58%\n",
            "Batch 3600/3750  | Loss: 0.3093   | Accuracy: 98.62%\n",
            "\n",
            "==================================================\n",
            "        \u001b[1m Epoch 6 Training Summary \u001b[0m        \n",
            "==================================================\n",
            "Train Loss              0.0589\n",
            "Train Accuracy          98.61%\n",
            "Test Loss               0.0351\n",
            "Test Accuracy           99.36%\n",
            "==================================================\n",
            "\n",
            "ðŸŒŸ Epoch 7/10\n",
            "Batch 300/3750  | Loss: 0.0229   | Accuracy: 98.73%\n",
            "Batch 600/3750  | Loss: 0.1574   | Accuracy: 98.62%\n",
            "Batch 900/3750  | Loss: 0.0150   | Accuracy: 98.71%\n",
            "Batch 1200/3750  | Loss: 0.0071   | Accuracy: 98.67%\n",
            "Batch 1500/3750  | Loss: 0.0037   | Accuracy: 98.64%\n",
            "Batch 1800/3750  | Loss: 0.0297   | Accuracy: 98.69%\n",
            "Batch 2100/3750  | Loss: 0.1349   | Accuracy: 98.65%\n",
            "Batch 2400/3750  | Loss: 0.0582   | Accuracy: 98.65%\n",
            "Batch 2700/3750  | Loss: 0.0046   | Accuracy: 98.63%\n",
            "Batch 3000/3750  | Loss: 0.1563   | Accuracy: 98.59%\n",
            "Batch 3300/3750  | Loss: 0.0289   | Accuracy: 98.63%\n",
            "Batch 3600/3750  | Loss: 0.0016   | Accuracy: 98.64%\n",
            "\n",
            "==================================================\n",
            "        \u001b[1m Epoch 7 Training Summary \u001b[0m        \n",
            "==================================================\n",
            "Train Loss              0.0597\n",
            "Train Accuracy          98.65%\n",
            "Test Loss               0.0374\n",
            "Test Accuracy           99.28%\n",
            "==================================================\n",
            "\n",
            "ðŸŒŸ Epoch 8/10\n",
            "Batch 300/3750  | Loss: 0.0084   | Accuracy: 98.62%\n",
            "Batch 600/3750  | Loss: 0.0012   | Accuracy: 98.62%\n",
            "Batch 900/3750  | Loss: 0.0050   | Accuracy: 98.70%\n",
            "Batch 1200/3750  | Loss: 0.0033   | Accuracy: 98.69%\n",
            "Batch 1500/3750  | Loss: 0.0002   | Accuracy: 98.69%\n",
            "Batch 1800/3750  | Loss: 0.0003   | Accuracy: 98.68%\n",
            "Batch 2100/3750  | Loss: 0.0094   | Accuracy: 98.69%\n",
            "Batch 2400/3750  | Loss: 0.2790   | Accuracy: 98.70%\n",
            "Batch 2700/3750  | Loss: 0.0015   | Accuracy: 98.69%\n",
            "Batch 3000/3750  | Loss: 0.1226   | Accuracy: 98.70%\n",
            "Batch 3300/3750  | Loss: 0.0094   | Accuracy: 98.69%\n",
            "Batch 3600/3750  | Loss: 0.1539   | Accuracy: 98.69%\n",
            "\n",
            "==================================================\n",
            "        \u001b[1m Epoch 8 Training Summary \u001b[0m        \n",
            "==================================================\n",
            "Train Loss              0.0588\n",
            "Train Accuracy          98.71%\n",
            "Test Loss               0.0407\n",
            "Test Accuracy           99.27%\n",
            "==================================================\n",
            "\n",
            "ðŸŒŸ Epoch 9/10\n",
            "Batch 300/3750  | Loss: 0.0824   | Accuracy: 98.73%\n",
            "Batch 600/3750  | Loss: 0.0013   | Accuracy: 98.68%\n",
            "Batch 900/3750  | Loss: 0.0023   | Accuracy: 98.71%\n",
            "Batch 1200/3750  | Loss: 0.0041   | Accuracy: 98.77%\n",
            "Batch 1500/3750  | Loss: 0.3288   | Accuracy: 98.71%\n",
            "Batch 1800/3750  | Loss: 0.0283   | Accuracy: 98.72%\n",
            "Batch 2100/3750  | Loss: 0.0007   | Accuracy: 98.73%\n",
            "Batch 2400/3750  | Loss: 0.0138   | Accuracy: 98.73%\n",
            "Batch 2700/3750  | Loss: 0.0021   | Accuracy: 98.72%\n",
            "Batch 3000/3750  | Loss: 0.2084   | Accuracy: 98.74%\n",
            "Batch 3300/3750  | Loss: 0.0013   | Accuracy: 98.73%\n",
            "Batch 3600/3750  | Loss: 0.0086   | Accuracy: 98.72%\n",
            "\n",
            "==================================================\n",
            "        \u001b[1m Epoch 9 Training Summary \u001b[0m        \n",
            "==================================================\n",
            "Train Loss              0.0563\n",
            "Train Accuracy          98.73%\n",
            "Test Loss               0.0424\n",
            "Test Accuracy           99.28%\n",
            "==================================================\n",
            "\n",
            "ðŸŒŸ Epoch 10/10\n",
            "Batch 300/3750  | Loss: 0.0040   | Accuracy: 98.73%\n",
            "Batch 600/3750  | Loss: 0.0764   | Accuracy: 98.74%\n",
            "Batch 900/3750  | Loss: 0.1373   | Accuracy: 98.70%\n",
            "Batch 1200/3750  | Loss: 0.0021   | Accuracy: 98.60%\n",
            "Batch 1500/3750  | Loss: 0.0015   | Accuracy: 98.64%\n",
            "Batch 1800/3750  | Loss: 0.0054   | Accuracy: 98.62%\n",
            "Batch 2100/3750  | Loss: 0.0001   | Accuracy: 98.62%\n",
            "Batch 2400/3750  | Loss: 0.0487   | Accuracy: 98.62%\n",
            "Batch 2700/3750  | Loss: 0.1754   | Accuracy: 98.59%\n",
            "Batch 3000/3750  | Loss: 0.0069   | Accuracy: 98.61%\n",
            "Batch 3300/3750  | Loss: 0.0012   | Accuracy: 98.61%\n",
            "Batch 3600/3750  | Loss: 0.0123   | Accuracy: 98.61%\n",
            "\n",
            "==================================================\n",
            "       \u001b[1m Epoch 10 Training Summary \u001b[0m        \n",
            "==================================================\n",
            "Train Loss              0.0602\n",
            "Train Accuracy          98.61%\n",
            "Test Loss               0.0388\n",
            "Test Accuracy           99.36%\n",
            "==================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [],
      "dockerImageVersionId": 30919,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
